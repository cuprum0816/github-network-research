#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ£€æŸ¥è´¡çŒ®è€…ç½‘ç»œæ•°æ®çš„åˆç†æ€§
åˆ†æä»“åº“è§„æ¨¡åˆ†å¸ƒå’Œè¾¹ç”Ÿæˆæƒ…å†µ
"""

import pandas as pd
import numpy as np
from collections import Counter

INPUT_FILE = "repo_to_contribs_top1000.parquet"
EDGES_FILE = "results/contributors_edges.parquet"


def analyze_repo_distribution():
    """åˆ†æä»“åº“çš„è´¡çŒ®è€…åˆ†å¸ƒ"""
    print("=" * 60)
    print("ä»“åº“è´¡çŒ®è€…åˆ†å¸ƒåˆ†æ")
    print("=" * 60)

    df = pd.read_parquet(INPUT_FILE)

    # ç»Ÿè®¡æ¯ä¸ªä»“åº“çš„è´¡çŒ®è€…æ•°é‡
    repo_sizes = []
    for _, row in df.iterrows():
        contribs = row["Contributors"]
        unique_contribs = len(set(str(u) for u in contribs))
        repo_sizes.append(unique_contribs)

    repo_sizes = np.array(repo_sizes)

    print(f"\nä»“åº“æ€»æ•°: {len(df)}")
    print(f"\nè´¡çŒ®è€…æ•°é‡ç»Ÿè®¡:")
    print(f"  å¹³å‡å€¼: {repo_sizes.mean():.1f}")
    print(f"  ä¸­ä½æ•°: {np.median(repo_sizes):.1f}")
    print(f"  æœ€å°å€¼: {repo_sizes.min()}")
    print(f"  æœ€å¤§å€¼: {repo_sizes.max()}")
    print(f"  æ ‡å‡†å·®: {repo_sizes.std():.1f}")

    # åˆ†ä½æ•°
    print(f"\nåˆ†ä½æ•°åˆ†å¸ƒ:")
    for q in [25, 50, 75, 90, 95, 99]:
        val = np.percentile(repo_sizes, q)
        print(f"  {q}%: {val:.0f}")

    # ç†è®ºè¾¹æ•°è®¡ç®—
    print(f"\nç†è®ºæœ€å¤§è¾¹æ•°è®¡ç®—:")
    theoretical_edges = sum([n * (n - 1) / 2 for n in repo_sizes])
    print(f"  å¦‚æœæ¯ä¸ªä»“åº“å†…æ‰€æœ‰äººä¸¤ä¸¤è¿è¾¹: {theoretical_edges:,.0f} æ¡")

    # æŒ‰è§„æ¨¡åˆ†ç»„
    print(f"\nä»“åº“è§„æ¨¡åˆ†ç»„:")
    bins = [0, 10, 50, 100, 200, 500, 1000, float('inf')]
    labels = ['1-10äºº', '11-50äºº', '51-100äºº', '101-200äºº',
              '201-500äºº', '501-1000äºº', '1000+äºº']

    repo_df = pd.DataFrame({'size': repo_sizes})
    repo_df['group'] = pd.cut(repo_df['size'], bins=bins, labels=labels)

    group_stats = repo_df.groupby('group', observed=True).agg({
        'size': ['count', 'mean', 'sum']
    }).round(1)

    print(group_stats)

    # è®¡ç®—æ¯ç»„ç†è®ºè´¡çŒ®çš„è¾¹æ•°
    print(f"\nå„è§„æ¨¡ç»„ç†è®ºè¾¹æ•°è´¡çŒ®:")
    for label in labels:
        group_repos = repo_df[repo_df['group'] == label]
        if len(group_repos) > 0:
            edges = sum([n * (n - 1) / 2 for n in group_repos['size']])
            print(f"  {label:15s}: {len(group_repos):4d} ä¸ªä»“åº“ â†’ {edges:12,.0f} æ¡è¾¹")

    return repo_sizes


def analyze_actual_network():
    """åˆ†æå®é™…ç”Ÿæˆçš„ç½‘ç»œ"""
    print("\n" + "=" * 60)
    print("å®é™…ç½‘ç»œåˆ†æ")
    print("=" * 60)

    edges_df = pd.read_parquet(EDGES_FILE)

    print(f"\nå®é™…è¾¹æ•°: {len(edges_df):,}")
    print(f"å®é™…èŠ‚ç‚¹æ•°: {len(set(edges_df['source']) | set(edges_df['target'])):,}")

    # æƒé‡åˆ†å¸ƒ
    print(f"\nè¾¹æƒé‡åˆ†å¸ƒ:")
    print(f"  å¹³å‡æƒé‡: {edges_df['weight'].mean():.2f}")
    print(f"  ä¸­ä½æ•°æƒé‡: {edges_df['weight'].median():.0f}")
    print(f"  æœ€å¤§æƒé‡: {edges_df['weight'].max()}")

    # æƒé‡åˆ†ä½æ•°
    print(f"\næƒé‡åˆ†ä½æ•°:")
    for q in [50, 75, 90, 95, 99]:
        val = edges_df['weight'].quantile(q / 100)
        print(f"  {q}%: {val:.0f}")

    # é«˜æƒé‡è¾¹
    print(f"\né«˜æƒé‡è¾¹ (weight >= 10):")
    high_weight = edges_df[edges_df['weight'] >= 10]
    print(f"  æ•°é‡: {len(high_weight):,} ({len(high_weight) / len(edges_df) * 100:.1f}%)")

    print(f"\nTop 10 é«˜æƒé‡è¾¹:")
    print(high_weight.nlargest(10, 'weight'))


def check_duplicate_contributors():
    """æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤è®¡ç®—çš„è´¡çŒ®è€…"""
    print("\n" + "=" * 60)
    print("è´¡çŒ®è€…é‡å¤æ€§æ£€æŸ¥")
    print("=" * 60)

    df = pd.read_parquet(INPUT_FILE)

    # ç»Ÿè®¡æ¯ä¸ªè´¡çŒ®è€…å‡ºç°åœ¨å¤šå°‘ä¸ªä»“åº“
    contributor_repos = Counter()
    total_contributions = 0

    for _, row in df.iterrows():
        contribs = [str(u) for u in row["Contributors"]]
        total_contributions += len(contribs)
        for user in set(contribs):
            contributor_repos[user] += 1

    print(f"\næ€»è´¡çŒ®è®°å½•æ•°: {total_contributions:,}")
    print(f"å”¯ä¸€è´¡çŒ®è€…æ•°: {len(contributor_repos):,}")
    print(f"å¹³å‡æ¯äººå‚ä¸ä»“åº“æ•°: {np.mean(list(contributor_repos.values())):.2f}")

    # æ´»è·ƒè´¡çŒ®è€…
    print(f"\nTop 10 æ´»è·ƒè´¡çŒ®è€… (å‚ä¸ä»“åº“æ•°):")
    for user, count in contributor_repos.most_common(10):
        print(f"  {user}: {count} ä¸ªä»“åº“")

    # åˆ†å¸ƒç»Ÿè®¡
    repo_counts = list(contributor_repos.values())
    print(f"\nè´¡çŒ®è€…æ´»è·ƒåº¦åˆ†å¸ƒ:")
    print(f"  ä¸­ä½æ•°: {np.median(repo_counts):.0f} ä¸ªä»“åº“")
    print(f"  å‚ä¸ >= 5 ä¸ªä»“åº“: {sum(1 for c in repo_counts if c >= 5):,} äºº")
    print(f"  å‚ä¸ >= 10 ä¸ªä»“åº“: {sum(1 for c in repo_counts if c >= 10):,} äºº")
    print(f"  å‚ä¸ >= 20 ä¸ªä»“åº“: {sum(1 for c in repo_counts if c >= 20):,} äºº")


def main():
    print("\n" + "=" * 60)
    print("è´¡çŒ®è€…ç½‘ç»œåˆç†æ€§æ£€æŸ¥")
    print("=" * 60)

    # åˆ†æä»“åº“åˆ†å¸ƒ
    repo_sizes = analyze_repo_distribution()

    # åˆ†æå®é™…ç½‘ç»œ
    analyze_actual_network()

    # æ£€æŸ¥è´¡çŒ®è€…é‡å¤æ€§
    check_duplicate_contributors()

    print("\n" + "=" * 60)
    print("åˆ†æå®Œæˆï¼")
    print("=" * 60)

    # ç»™å‡ºç»“è®º
    print("\nğŸ’¡ ç»“è®º:")
    print("  1. å¦‚æœå¤§å¤šæ•°ä»“åº“æœ‰100+è´¡çŒ®è€…ï¼Œ246ä¸‡æ¡è¾¹æ˜¯åˆç†çš„")
    print("  2. é«˜æƒé‡è¾¹è¡¨ç¤ºæ ¸å¿ƒåä½œå›¢é˜Ÿï¼ˆåŒæ—¶å‚ä¸å¤šä¸ªé¡¹ç›®ï¼‰")
    print("  3. å¦‚æœè§‰å¾—è¾¹å¤ªå¤šï¼Œå¯ä»¥è€ƒè™‘:")
    print("     - è¿‡æ»¤ä½æƒé‡è¾¹ (weight < 2)")
    print("     - åªä¿ç•™å¤´éƒ¨æ´»è·ƒè´¡çŒ®è€…")
    print("     - é™åˆ¶å•ä¸ªä»“åº“çš„æœ€å¤§è´¡çŒ®è€…æ•°é‡")


if __name__ == "__main__":
    main()